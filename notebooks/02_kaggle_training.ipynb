{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Kaggle Training: Place Recognition on NCLT\n",
    "\n",
    "Train a MinkLoc3D-based place recognition model on the NCLT dataset.\n",
    "\n",
    "**Requirements**: Attach the [NCLT dataset](https://www.kaggle.com/datasets/creatorofuniverses/nclt-iprofi-hack-23) to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the project repo (adjust URL)\n",
    "# !git clone https://github.com/YOUR_USERNAME/nclt-slam-project.git\n",
    "# %cd nclt-slam-project\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchvision open3d pyyaml tqdm scipy scikit-learn matplotlib\n",
    "# MinkowskiEngine requires special installation:\n",
    "# !pip install -q MinkowskiEngine -f https://nvidia.com/MinkowskiEngine/cu118/torch2.0.0/index.html\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Link Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "KAGGLE_DATA = Path('/kaggle/input/nclt-iprofi-hack-23/NCLT_preprocessed')\n",
    "LOCAL_DATA = Path('./data/NCLT_preprocessed')\n",
    "\n",
    "if KAGGLE_DATA.exists():\n",
    "    print(f'Running on Kaggle. Data at: {KAGGLE_DATA}')\n",
    "    # Create symlink for unified access\n",
    "    LOCAL_DATA.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not LOCAL_DATA.exists():\n",
    "        os.symlink(str(KAGGLE_DATA), str(LOCAL_DATA))\n",
    "    data_path = KAGGLE_DATA\n",
    "elif LOCAL_DATA.exists():\n",
    "    print(f'Running locally. Data at: {LOCAL_DATA}')\n",
    "    data_path = LOCAL_DATA\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        'Dataset not found. On Kaggle, attach the NCLT dataset. '\n",
    "        'Locally, run: python scripts/download_nclt_sample.py'\n",
    "    )\n",
    "\n",
    "# List available sessions\n",
    "sessions_dir = data_path / 'sessions'\n",
    "if sessions_dir.exists():\n",
    "    sessions = sorted([d.name for d in sessions_dir.iterdir() if d.is_dir()])\n",
    "    print(f'Available sessions ({len(sessions)}): {sessions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.utils.point_cloud import load_velodyne_bin\n",
    "from src.utils.io_utils import load_config\n",
    "\n",
    "config = load_config('configs/dataset_config.yaml')\n",
    "\n",
    "# Load a sample point cloud\n",
    "sample_session = sessions[0] if sessions else '2012-01-08'\n",
    "velodyne_dir = data_path / 'sessions' / sample_session / 'velodyne'\n",
    "\n",
    "if velodyne_dir.exists():\n",
    "    bin_files = sorted(velodyne_dir.glob('*.bin'))\n",
    "    if bin_files:\n",
    "        sample_pc = load_velodyne_bin(bin_files[0])\n",
    "        print(f'Sample point cloud: {sample_pc.shape}')\n",
    "        print(f'X range: [{sample_pc[:,0].min():.1f}, {sample_pc[:,0].max():.1f}]')\n",
    "        print(f'Y range: [{sample_pc[:,1].min():.1f}, {sample_pc[:,1].max():.1f}]')\n",
    "        print(f'Z range: [{sample_pc[:,2].min():.1f}, {sample_pc[:,2].max():.1f}]')\n",
    "        \n",
    "        # Visualize\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.scatter(sample_pc[:, 0], sample_pc[:, 1], s=0.1, c=sample_pc[:, 2], cmap='viridis')\n",
    "        ax.set_xlabel('X (m)')\n",
    "        ax.set_ylabel('Y (m)')\n",
    "        ax.set_title(f'Sample Point Cloud - {sample_session}')\n",
    "        ax.set_aspect('equal')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No .bin files found')\n",
    "else:\n",
    "    print(f'Velodyne directory not found: {velodyne_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = load_config('configs/train_config.yaml')\n",
    "training = train_config['training']\n",
    "\n",
    "print('Training configuration:')\n",
    "for key, value in training.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Train the place recognition model with triplet loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Choose model based on MinkowskiEngine availability\n",
    "try:\n",
    "    import MinkowskiEngine as ME\n",
    "    model_type = 'minkloc3d'\n",
    "    print('MinkowskiEngine available - using MinkLoc3D')\n",
    "except ImportError:\n",
    "    model_type = 'pointnet'\n",
    "    print('MinkowskiEngine not available - falling back to PointNet')\n",
    "\n",
    "from src.models.place_recognition import PlaceRecognitionWrapper, TripletLoss\n",
    "\n",
    "feature_dim = training.get('feature_dim', 256)\n",
    "model = PlaceRecognitionWrapper(\n",
    "    model_type=model_type,\n",
    "    feature_dim=feature_dim,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = TripletLoss(margin=training.get('margin', 0.2))\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=training.get('learning_rate', 1e-3),\n",
    "    weight_decay=training.get('weight_decay', 1e-4),\n",
    ")\n",
    "\n",
    "print(f'Model: {model_type}, feature_dim={feature_dim}')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement actual training loop\n",
    "# This is a placeholder - fill in after data loading is verified\n",
    "\n",
    "# from src.datasets.nclt_pairs import NCLTPairsDataset, pairs_collate_fn\n",
    "# from src.datasets.transforms import build_transforms\n",
    "#\n",
    "# aug_config = {\n",
    "#     'point_cloud': config['nclt']['point_cloud'],\n",
    "#     'augmentation': training.get('augmentation', {}),\n",
    "# }\n",
    "# train_transform = build_transforms(aug_config, is_train=True)\n",
    "#\n",
    "# train_dataset = NCLTPairsDataset(\n",
    "#     config_path='configs/dataset_config.yaml',\n",
    "#     split='train',\n",
    "#     transform=train_transform,\n",
    "# )\n",
    "#\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=training.get('batch_size', 32),\n",
    "#     shuffle=True,\n",
    "#     num_workers=2,\n",
    "#     collate_fn=pairs_collate_fn,\n",
    "# )\n",
    "#\n",
    "# epochs = training.get('epochs', 80)\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0.0\n",
    "#     for batch in train_loader:\n",
    "#         anchor_desc = model(batch['anchor'].to(device))\n",
    "#         positive_desc = model(batch['positive'].to(device))\n",
    "#         negative_desc = model(batch['negatives'].to(device))\n",
    "#         loss = loss_fn(anchor_desc, positive_desc, negative_desc)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "#     print(f'Epoch {epoch}: loss={epoch_loss/len(train_loader):.4f}')\n",
    "\n",
    "print('Training loop placeholder - uncomment when data is ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.evaluation.metrics import recall_at_k\n",
    "# from src.evaluation.visualization import plot_recall_at_k\n",
    "#\n",
    "# # Extract descriptors for validation set\n",
    "# model.eval()\n",
    "# descriptors = []\n",
    "# positions = []\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     for batch in val_loader:\n",
    "#         desc = model(batch['anchor'].to(device))\n",
    "#         descriptors.append(desc.cpu().numpy())\n",
    "#         positions.append(batch['anchor_pose'][:, :3, 3].numpy())\n",
    "#\n",
    "# descriptors = np.concatenate(descriptors)\n",
    "# positions = np.concatenate(positions)\n",
    "#\n",
    "# # Compute recall@K\n",
    "# n = len(descriptors)\n",
    "# mid = n // 2\n",
    "# recall = recall_at_k(\n",
    "#     descriptors[:mid], descriptors[mid:],\n",
    "#     positions[:mid], positions[mid:],\n",
    "#     k_values=[1, 5, 10],\n",
    "# )\n",
    "# print(f'Recall: {recall}')\n",
    "# plot_recall_at_k(recall)\n",
    "# plt.show()\n",
    "\n",
    "print('Evaluation placeholder - uncomment after training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Kaggle output\n",
    "import os\n",
    "\n",
    "output_dir = Path('/kaggle/working') if Path('/kaggle/working').exists() else Path('./checkpoints')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'config': training,\n",
    "# }, output_dir / 'best_model.pth')\n",
    "# print(f'Model saved to {output_dir / \"best_model.pth\"}')\n",
    "\n",
    "print(f'Checkpoint directory: {output_dir}')\n",
    "print('Uncomment save code after training is implemented')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
